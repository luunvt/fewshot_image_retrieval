{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cat = '3848'\n",
    "inference_image_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3848/3848\"\n",
    "focus_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3848/focus_3848\"\n",
    "additional_focus_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3848/additional_focus\"\n",
    "yes_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/3848/fewshot_additional_focus/yes\"\n",
    "no_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/3848/fewshot_additional_focus/no\"\n",
    "detect_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/3848/fewshot_additional_focus/detect\"\n",
    "\n",
    "config = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/additional_focus/3848/attention-rpn_r50_c4_voc-split1_5shot-fine-tuning.py\"\n",
    "checkpoint = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/additional_focus/3848/iter_1200.pth\"\n",
    "support_images_dir = '/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/demo/demo_detection_images/support_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def get_max_object(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    ## (2) Morph-op to remove noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    ## (3) Find the max-area contour\n",
    "    cnts = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    for cnt in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        x1.append(x)\n",
    "        y1.append(y)\n",
    "        x2.append(x+w)\n",
    "        y2.append(y+h)\n",
    "    x1 = min(x1)\n",
    "    y1 = min(y1)\n",
    "    x2 = max(x2)\n",
    "    y2 = max(y2)\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 16:25:44.302064: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-22 16:25:44.327398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-22 16:25:44.691860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-22 16:25:45.107776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.122285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.122370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.123300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.123387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.123438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.468452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.468555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.468615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-22 16:25:45.468654: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2023-05-22 16:25:45.468669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9204 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet import ResNet152, preprocess_input\n",
    "import keras.utils as image\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "model_embedding = ResNet152(include_top=False, weights='imagenet', pooling='avg')\n",
    "def return_image_embedding(img):\n",
    "    if isinstance(img, str):\n",
    "        img = image.load_img(img, target_size=(224, 224))\n",
    "    else:\n",
    "        img = img.resize((224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    preds = model_embedding.predict(x, verbose=False)\n",
    "    preds = preds[0].T\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "model_vit = vit_b_16(weights=weights)\n",
    "model_vit.eval()\n",
    "\n",
    "return_nodes = {\n",
    "    \"getitem_5\": \"output\"\n",
    "}\n",
    "model_vit = create_feature_extractor(model_vit, return_nodes=return_nodes)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "def return_image_embedding_VIT(img):\n",
    "    if isinstance(img, str):\n",
    "        img = read_image(img)\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "    preds = model_vit(batch)['output'].squeeze(0).detach().numpy()\n",
    "    return preds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2616/2616 [01:51<00:00, 23.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "THR = 0.55\n",
    "\n",
    "focus_embedding = []\n",
    "for img_focus in os.listdir(focus_path):\n",
    "  img1 = os.path.join(focus_path, img_focus)\n",
    "  x1, y1, x2, y2 = get_max_object(img1)\n",
    "  img_embed = return_image_embedding(model_embedding,img1)\n",
    "  focus_embedding.append(img_embed)\n",
    "\n",
    "for img_name in tqdm(os.listdir(inference_image_path)):\n",
    "  img = os.path.join(inference_image_path, img_name)\n",
    "  img_embed = return_image_embedding(model_embedding, img)\n",
    "  scr = [round(1- distance.cosine(img_embed,x), 2) for x in focus_embedding]\n",
    "  if any(x >= THR for x in scr):\n",
    "    shutil.copy(os.path.join(inference_image_path, img_name), os.path.join(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/door_compare_focus_background/embedding_only/yes\", img_name))\n",
    "    # continue\n",
    "  else:\n",
    "    shutil.copy(os.path.join(inference_image_path, img_name), os.path.join(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/door_compare_focus_background/embedding_only/no\", img_name))\n",
    "    # continue\n",
    "# img = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/test_img.png\"\n",
    "# img_embed = return_image_embedding(model_embedding,img)\n",
    "# scr = [round(1- distance.cosine(img_embed,x), 2) for x in focus_embedding]\n",
    "# print(scr)\n",
    "# print(sum(scr) / len(scr))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/additional_focus/3848/iter_1200.pth\n",
      "cuda:0\n",
      "True\n",
      "FOCUS\n",
      "B09LHC752N.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 16:25:55.490141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-05-22 16:25:55.537962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B07PDQFDMF.jpg\n",
      "B07WFMJ84J.jpg\n",
      "B0734MYZPC.jpg\n",
      "B07WN1874X.jpg\n",
      "ADDITIONAL FOCUS\n",
      "B0B4SJ29N6.jpg\n",
      "B0B2V6QR96.jpg\n",
      "B0BB8Z6585.jpg\n",
      "B0B8PGM7H4.jpg\n",
      "B0747L2VLZ.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8027/8027 [07:32<00:00, 17.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\"\"\"Inference Attention RPN Detector with support instances.\n",
    "\"\"\"  # nowq\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from mmdet.apis import show_result_pyplot\n",
    "\n",
    "from mmfewshot.detection.apis import (inference_detector, init_detector,\n",
    "                                    process_support_images)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "import keras.utils as image\n",
    "import shutil\n",
    "from mmdet.apis import show_result_pyplot\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser('attention rpn inference.')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def nms(boxes, thr):\n",
    "  boxbox = torch.from_numpy(boxes[:, 0:4])\n",
    "  score = torch.from_numpy(boxes[:, 4])\n",
    "  result = torchvision.ops.nms(boxbox, score, thr)\n",
    "  return result.numpy()\n",
    "\n",
    "def get_ebedd_origin(model_embedding, image):\n",
    "    cropped_embed = return_image_embedding(model_embedding, image)\n",
    "    scr = [round(1- distance.cosine(cropped_embed, x), 2) for x in focus_embedding]\n",
    "    print(scr)\n",
    "    print(sum(scr) / len(scr))\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "include = []\n",
    "exclude = []\n",
    "\n",
    "device = 'cuda:0'\n",
    "score_thr = 0.1\n",
    "SIMILAR_THR = 0.55\n",
    "model = init_detector(config, checkpoint, device=device)\n",
    "print(next(model.parameters()).device)\n",
    "print(next(model.parameters()).is_cuda)\n",
    "\n",
    "# prepare support images, each demo image only contain one instance\n",
    "files = os.listdir(support_images_dir)\n",
    "support_images = [\n",
    "    os.path.join(support_images_dir, file) for file in files\n",
    "]\n",
    "\n",
    "classes = [file.split('.')[0] for file in files]\n",
    "support_labels = [[file.split('.')[0]] for file in files]\n",
    "process_support_images(model, support_images, support_labels, classes=classes)\n",
    "\n",
    "focus_embedding = []\n",
    "print(\"FOCUS\")\n",
    "for img_focus in os.listdir(focus_path):\n",
    "  print(img_focus)\n",
    "  img_path = os.path.join(focus_path, img_focus)\n",
    "  img = image.load_img(img_path, target_size=(224, 224))\n",
    "  img_embed = return_image_embedding(img)\n",
    "  focus_embedding.append(img_embed)\n",
    "  \n",
    "additional_embedding = []\n",
    "additional_crop_embedding = []\n",
    "print(\"ADDITIONAL FOCUS\")\n",
    "for img_focus in os.listdir(additional_focus_path):\n",
    "  print(img_focus)\n",
    "  img_path = os.path.join(additional_focus_path, img_focus)\n",
    "  additional_embedding.append(return_image_embedding(img_path))\n",
    "  x1, y1, x2, y2 = get_max_object(img_path)\n",
    "  img = image.load_img(img_path)\n",
    "  img_cropped = img.crop((x1, y1, x2, y2))\n",
    "  crop_embedd = return_image_embedding(img_cropped)\n",
    "  additional_crop_embedding.append(crop_embedd)\n",
    "\n",
    "for image_name in tqdm(os.listdir(inference_image_path)):\n",
    "# for image_name in [\"B00AIIFR2U.jpg\"]:\n",
    "    image_path = os.path.join(inference_image_path, image_name)\n",
    "    img_embed = return_image_embedding(image_path)\n",
    "    scr = [round(1- distance.cosine(img_embed,x), 2) for x in focus_embedding + additional_embedding]\n",
    "    # print(scr)\n",
    "    if any(x >= SIMILAR_THR for x in scr):\n",
    "        shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(yes_path, image_name))\n",
    "        include.append(image_name)\n",
    "    else:\n",
    "        try:\n",
    "            result = inference_detector(model, image_path)\n",
    "        except:\n",
    "            print(image_path)\n",
    "            continue\n",
    "        res = sorted([x for x in result[0] if x[4] >= score_thr], key=lambda x: x[4], reverse=True)\n",
    "        if len(res) > 0:\n",
    "            list_scr = []\n",
    "            for bbox in res[0:1]:\n",
    "                img = image.load_img(image_path)\n",
    "                cropped = img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "                # cropped.save('./test_img.png')\n",
    "                # result = inference_detector(model, './test_img.png')\n",
    "                # print(\"cropped: \", *result)\n",
    "                # show_result_pyplot(model, './test_img.png', result, score_thr=score_thr)\n",
    "                cropped_embed = return_image_embedding(cropped)\n",
    "                scr = [round(1- distance.cosine(cropped_embed, x), 2) for x in additional_crop_embedding]\n",
    "                # scr2 = [round(1- distance.cosine(cropped_embed, x), 2) for x in focus_embedding]\n",
    "                list_scr.append(max(scr))\n",
    "                # print(scr)\n",
    "                # print(scr2)\n",
    "            if max(list_scr) >= SIMILAR_THR:\n",
    "                shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(yes_path, image_name))\n",
    "                output_path = os.path.join(detect_path, image_name)\n",
    "                custom_res = np.expand_dims(np.expand_dims(np.array(res[0:1][0]), 0), 0)\n",
    "                model.show_result(image_path, custom_res, out_file=output_path, score_thr=score_thr)\n",
    "                include.append(image_name)\n",
    "            else:\n",
    "                shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(no_path, image_name))\n",
    "                exclude.append(image_name)\n",
    "        else:\n",
    "            shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(no_path, image_name))\n",
    "            exclude.append(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8027\n",
      "7477\n",
      "550\n",
      "593\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(inference_image_path)))\n",
    "print(len(os.listdir(yes_path)))\n",
    "print(len(os.listdir(no_path)))\n",
    "print(len(os.listdir(detect_path)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection + Embedding Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.spatial import distance\n",
    "\n",
    "# from torchvision.io import read_image\n",
    "# from torchvision.models import resnet50, ResNet50_Weights, vit_b_16, ViT_B_16_Weights\n",
    "# import torch.nn as nn\n",
    "# from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "\n",
    "# img = read_image(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/img/focus_door/no_background/B0B8VT3225.jpg\")\n",
    "# print(img)\n",
    "# # Step 1: Initialize model with the best available weights\n",
    "# weights = ViT_B_16_Weights.DEFAULT\n",
    "# model_vit = vit_b_16(weights=weights)\n",
    "# model_vit.eval()\n",
    "# # train_nodes, eval_nodes = get_graph_node_names(model_vit)\n",
    "# # print(eval_nodes)\n",
    "# # print(model_vit)\n",
    "\n",
    "# return_nodes = {\n",
    "#     \"getitem_5\": \"output\"\n",
    "# }\n",
    "\n",
    "# model_vit = create_feature_extractor(model_vit, return_nodes=return_nodes)\n",
    "\n",
    "# # Step 2: Initialize the inference transforms\n",
    "# preprocess = weights.transforms()\n",
    "\n",
    "# # Step 3: Apply inference preprocessing transforms\n",
    "# batch = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# # Step 4: Use the model_vit and print the predicted category\n",
    "# prediction = model_vit(batch)['output'].squeeze(0).detach().numpy()\n",
    "# # print(prediction.shape)\n",
    "# print(round(1- distance.cosine(prediction,prediction), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "weights = ViT_B_16_Weights.DEFAULT\n",
    "model_vit = vit_b_16(weights=weights)\n",
    "model_vit.eval()\n",
    "\n",
    "return_nodes = {\n",
    "    \"getitem_5\": \"output\"\n",
    "}\n",
    "model_vit = create_feature_extractor(model_vit, return_nodes=return_nodes)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "def return_image_cropped_embedding_VIT(img, x1=None, y1=None, x2=None, y2=None):\n",
    "    if isinstance(img, str):\n",
    "        img = read_image(img)\n",
    "    if x1:\n",
    "        img = img[y1:y2, x1:x2]\n",
    "    batch = preprocess(img).unsqueeze(0)\n",
    "    preds = model_vit(batch)['output'].squeeze(0).detach().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/kettlebell_attention-rpn_5shot-fine-tuning/iter_1200.pth\n",
      "cuda:0\n",
      "True\n",
      "FOCUS\n",
      "5.jpg\n",
      "4.jpg\n",
      "71rZtVP7ZPL.jpg\n",
      "3.jpg\n",
      "2.jpg\n",
      "ADDITIONAL FOCUS\n",
      "B00A9HEPQY.jpg\n",
      "B0B74V7Z8F.jpg\n",
      "B0B4N78HKM.jpg\n",
      "B0B2ZG4TRQ.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5326/5326 [08:59<00:00,  9.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "\"\"\"Inference Attention RPN Detector with support instances.\n",
    "\"\"\"  # nowq\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from mmdet.apis import show_result_pyplot\n",
    "\n",
    "from mmfewshot.detection.apis import (inference_detector, init_detector,\n",
    "                                    process_support_images)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "from scipy.spatial import distance\n",
    "import keras.utils as image\n",
    "import shutil\n",
    "from mmdet.apis import show_result_pyplot\n",
    "\n",
    "def parse_args():\n",
    "    parser = ArgumentParser('attention rpn inference.')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def nms(boxes, thr):\n",
    "  boxbox = torch.from_numpy(boxes[:, 0:4])\n",
    "  score = torch.from_numpy(boxes[:, 4])\n",
    "  result = torchvision.ops.nms(boxbox, score, thr)\n",
    "  return result.numpy()\n",
    "\n",
    "def get_ebedd_origin(model_embedding, image):\n",
    "    cropped_embed = return_image_embedding(model_embedding, image)\n",
    "    scr = [round(1- distance.cosine(cropped_embed, x), 2) for x in focus_embedding]\n",
    "    print(scr)\n",
    "    print(sum(scr) / len(scr))\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "include = []\n",
    "exclude = []\n",
    "\n",
    "device = 'cuda:0'\n",
    "score_thr = 0.2\n",
    "SIMILAR_THR = 0.55\n",
    "CROPPED_THR = 0.61\n",
    "THR_MAX_KNN = 0.7\n",
    "THR_MIN_KNN = 0.4\n",
    "\n",
    "model = init_detector(config, checkpoint, device=device)\n",
    "print(next(model.parameters()).device)\n",
    "print(next(model.parameters()).is_cuda)\n",
    "\n",
    "# prepare support images, each demo image only contain one instance\n",
    "files = os.listdir(support_images_dir)\n",
    "support_images = [\n",
    "    os.path.join(support_images_dir, file) for file in files\n",
    "]\n",
    "\n",
    "classes = [file.split('.')[0] for file in files]\n",
    "support_labels = [[file.split('.')[0]] for file in files]\n",
    "process_support_images(model, support_images, support_labels, classes=classes)\n",
    "\n",
    "focus_embedding = []\n",
    "focus_cropped_embedding = []\n",
    "print(\"FOCUS\")\n",
    "for img_focus in os.listdir(focus_path):\n",
    "  print(img_focus)\n",
    "  img1 = os.path.join(focus_path, img_focus)\n",
    "  img_embed = return_image_embedding(model_embedding, img1)\n",
    "  focus_embedding.append(img_embed)\n",
    "\n",
    "  x1, y1, x2, y2 = get_max_object(img1)\n",
    "  crop_embedd = return_image_cropped_embedding(model_embedding, img1, x1, y1, x2, y2)\n",
    "  focus_cropped_embedding.append(crop_embedd)\n",
    "\n",
    "additional_embedding = []\n",
    "additional_crop_embedding = []\n",
    "print(\"ADDITIONAL FOCUS\")\n",
    "for img_focus in os.listdir(additional_focus_path):\n",
    "  print(img_focus)\n",
    "  img1_path = os.path.join(additional_focus_path, img_focus)\n",
    "  img_embed = return_image_embedding(model_embedding, img1_path)\n",
    "  additional_embedding.append(img_embed)\n",
    "\n",
    "  x1, y1, x2, y2 = get_max_object(img1_path)\n",
    "  crop_embedd = return_image_cropped_embedding(model_embedding, img1_path, x1, y1, x2, y2)\n",
    "  additional_crop_embedding.append(crop_embedd)\n",
    "\n",
    "all_img_name_to_embedding = {}\n",
    "embedding_to_image_name = []\n",
    "x_train_knn = []\n",
    "y_train_knn = []\n",
    "for image_name in tqdm(os.listdir(inference_image_path)):\n",
    "# for image_name in [\"B092SD9GKY.jpg\"]:\n",
    "    image_path = os.path.join(inference_image_path, image_name)\n",
    "    img_embed = return_image_embedding(model_embedding, image_path)\n",
    "    scr = [round(1- distance.cosine(img_embed,x), 2) for x in focus_embedding]\n",
    "    if any(x >= SIMILAR_THR for x in scr):\n",
    "        shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(yes_path, image_name))\n",
    "        include.append(image_name)\n",
    "        if max(scr) >= THR_MAX_KNN:\n",
    "            embedding_to_image_name.append(image_name)\n",
    "            x_train_knn.append(img_embed)\n",
    "            y_train_knn.append(1)\n",
    "        all_img_name_to_embedding[image_name] = img_embed\n",
    "    else:\n",
    "        try:\n",
    "            result = inference_detector(model, image_path)\n",
    "        except:\n",
    "            print(image_path)\n",
    "            continue\n",
    "        res = sorted([x for x in result[0] if x[4] >= score_thr], key=lambda x: x[4], reverse=True)\n",
    "        if len(res) > 0:\n",
    "            list_scr = []\n",
    "            for bbox in res[0:1]:\n",
    "                img = image.load_img(image_path)\n",
    "                cropped = img.crop((bbox[0], bbox[1], bbox[2], bbox[3]))\n",
    "                cropped.save('./test_img.png')\n",
    "                # result = inference_detector(model, './test_img.png')\n",
    "                # print(\"cropped: \", *result)\n",
    "                # show_result_pyplot(model, './test_img.png', result, score_thr=score_thr)\n",
    "                cropped_embed = return_image_embedding(model_embedding, cropped)\n",
    "                scr = [round(1- distance.cosine(cropped_embed, x), 2) for x in additional_crop_embedding]\n",
    "                list_scr.append(max(scr))\n",
    "            if max(list_scr) >= SIMILAR_THR:\n",
    "                shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(yes_path, image_name))\n",
    "                output_path = os.path.join(detect_path, image_name)\n",
    "                custom_res = np.expand_dims(np.expand_dims(np.array(res[0:1][0]), 0), 0)\n",
    "                model.show_result(image_path, custom_res, out_file=output_path, score_thr=score_thr)\n",
    "                include.append(image_name)\n",
    "                if max(list_scr) >= THR_MAX_KNN:\n",
    "                    embedding_to_image_name.append(image_name)\n",
    "                    x_train_knn.append(cropped_embed)\n",
    "                    y_train_knn.append(1)\n",
    "                all_img_name_to_embedding[image_name] = cropped_embed\n",
    "            else:\n",
    "                shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(no_path, image_name))\n",
    "                exclude.append(image_name)\n",
    "                if max(list_scr) <= THR_MIN_KNN:\n",
    "                    embedding_to_image_name.append(image_name)\n",
    "                    x_train_knn.append(cropped_embed)\n",
    "                    y_train_knn.append(0)\n",
    "                all_img_name_to_embedding[image_name] = cropped_embed\n",
    "        else:\n",
    "            shutil.copy(os.path.join(inference_image_path, image_name), os.path.join(no_path, image_name))\n",
    "            exclude.append(image_name)\n",
    "            if max(scr) <= THR_MIN_KNN:\n",
    "                embedding_to_image_name.append(image_name)\n",
    "                x_train_knn.append(img_embed)\n",
    "                y_train_knn.append(0)\n",
    "            all_img_name_to_embedding[image_name] = img_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653 2653 2653\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_knn), len(y_train_knn), len(embedding_to_image_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(yes_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(x_train_knn, y_train_knn)\n",
    "mat = [all_img_name_to_embedding[x] for x in os.listdir(yes_path)]\n",
    "remove = []\n",
    "path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/kettlebell/fewshot_additional_focus_knn/eliminate_40_70\"\n",
    "for img_name in os.listdir(yes_path):\n",
    "    embedd = [all_img_name_to_embedding[img_name]]\n",
    "    res = neigh.predict(embedd)\n",
    "    if res.item() == 0:\n",
    "        shutil.copy(os.path.join(inference_image_path, img_name), os.path.join(path, img_name))\n",
    "# print(len(remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_train_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "path_high = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/kettlebell/fewshot_additional_focus_knn/higher_than_80\"\n",
    "path_low = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/kettlebell/fewshot_additional_focus_knn/lower_than_80\"\n",
    "\n",
    "for i in range(len(embedding_to_image_name)):\n",
    "    # print(y_train_knn[i] == 1)\n",
    "    if y_train_knn[i] == 1:\n",
    "        shutil.copy(os.path.join(inference_image_path, embedding_to_image_name[i]), os.path.join(path_high, embedding_to_image_name[i]))\n",
    "    else:\n",
    "        shutil.copy(os.path.join(inference_image_path, embedding_to_image_name[i]), os.path.join(path_low, embedding_to_image_name[i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2616\n",
      "Embedd MISS:  344\n",
      "Embedd REDUNDANT:  223\n",
      "MISS:  1068\n",
      "REDUNDANT:  0\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(inference_image_path)))\n",
    "list_true_include = os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/door_gt/door\")\n",
    "list_embedding = os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/door/embedding_only/yes\")\n",
    "embedding_miss = [x for x in list_true_include if x not in list_embedding]\n",
    "embedding_redundant = [x for x in list_embedding if x not in list_true_include]\n",
    "miss = [x for x in list_true_include if x not in include]   \n",
    "redundant = [x for x in include if x not in list_true_include]\n",
    "\n",
    "print(\"Embedd MISS: \", len(embedding_miss))\n",
    "print(\"Embedd REDUNDANT: \", len(embedding_redundant))\n",
    "print(\"MISS: \", len(miss))\n",
    "print(\"REDUNDANT: \", len(redundant))\n",
    "# a = len(exclude)\n",
    "# b = len(os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/door_gt/not_door\"))\n",
    "# print(\"MISS: \", a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "path_1 = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/kettlebell/fewshot/detect\"\n",
    "path_2 = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/kettlebell/fewshot_additional_focus/detect\"\n",
    "diff = list(set(os.listdir(path_1)) - set(os.listdir(path_2)))\n",
    "print(len(diff))\n",
    "# x = [img_name for image_name in include if image_name not in embedding_miss and img_name in list_true_include]\n",
    "# print(len(x))\n",
    "for img_name in diff:\n",
    "    # if img_name in list_true_include:\n",
    "    shutil.copy(os.path.join(inference_image_path, img_name), \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/kettlebell/diff_no_add_vs_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "diff = list(set(redundant) - set(embedding_redundant))\n",
    "print(len(diff))\n",
    "# x = [img_name for image_name in include if image_name not in embedding_miss and img_name in list_true_include]\n",
    "# print(len(x))\n",
    "for img_name in miss:\n",
    "    # if img_name in list_true_include:\n",
    "    shutil.copy(os.path.join(inference_image_path, img_name), \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/door/fewshot/still_missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7379\n",
      "7378\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "path1 = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/3848/fewshot/yes\"\n",
    "path2 = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/3848/embedding/yes\"\n",
    "path_similar = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3884/similar_3884\"\n",
    "\n",
    "path1_list = os.listdir(path1)\n",
    "path2_list = os.listdir(path2)\n",
    "path_similar_list = os.listdir(path_similar)\n",
    "print(len(path1_list))\n",
    "print(len(path2_list))\n",
    "# print(len(path_similar_list))\n",
    "\n",
    "diff = list(set(path1_list) - set(path2_list))\n",
    "print(len(diff))\n",
    "for img_name in diff:\n",
    "    # if image_name not in path1_list:\n",
    "    shutil.copy(os.path.join(inference_image_path, img_name),\n",
    "                os.path.join(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/3848/few-embedd\", img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1061\n",
      "1554\n",
      "Acc:  0.7989296636085627\n",
      "Precision:  0.7625122669283612\n",
      "Recall:  0.7323279924599434\n",
      "F1:  0.7471153846153846\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "all_images = os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/img/door\")\n",
    "true_list = os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/door_gt/door\")\n",
    "false_list = os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/door_gt/not_door\")\n",
    "print(len(true_list))\n",
    "print(len(false_list))\n",
    "y_true = [1 if img_name in true_list else 0 for img_name in all_images]\n",
    "\n",
    "predictions = os.listdir(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_fewshot_retrieval/door/fewshot/yes\")\n",
    "y_predict = [1 if img_name in predictions else 0 for img_name in all_images]\n",
    "\n",
    "print(\"Acc: \", accuracy_score(y_true, y_predict))\n",
    "print(\"Precision: \", precision_score(y_true, y_predict))\n",
    "print(\"Recall: \", recall_score(y_true, y_predict))\n",
    "print(\"F1: \", f1_score(y_true, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmfewshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
