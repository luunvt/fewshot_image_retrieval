{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmfewshot\n",
    "import mmdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = [5]\n",
    "xml_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/data/VOCdevkit/VOC2012/Annotations\"\n",
    "image_xml_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/data/VOCdevkit/VOC2012/JPEGImages\"\n",
    "few_shot_ann_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/data/few_shot_ann/voc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_xml_content(cat, img_path, x1, y1, x2, y2):\n",
    "  img_name = img_path.split(\"/\")[-1]\n",
    "  img = Image.open(img_path)\n",
    "  w, h = img.size\n",
    "  content = f'''\n",
    "<annotation>\n",
    "<filename>{img_name}</filename>\n",
    "<folder>VOC2012</folder>\n",
    "<object>\n",
    "  <name>{cat}</name>\n",
    "  <actions>\n",
    "    <jumping>0</jumping>\n",
    "    <other>1</other>\n",
    "    <phoning>0</phoning>\n",
    "    <playinginstrument>0</playinginstrument>\n",
    "    <reading>0</reading>\n",
    "    <ridingbike>0</ridingbike>\n",
    "    <ridinghorse>0</ridinghorse>\n",
    "    <running>0</running>\n",
    "    <takingphoto>0</takingphoto>\n",
    "    <usingcomputer>0</usingcomputer>\n",
    "    <walking>0</walking>\n",
    "  </actions>\n",
    "  <bndbox>\n",
    "    <xmax>{x2}</xmax>\n",
    "    <xmin>{x1}</xmin>\n",
    "    <ymax>{y2}</ymax>\n",
    "    <ymin>{y1}</ymin>\n",
    "  </bndbox>\n",
    "  <difficult>0</difficult>\n",
    "  <pose>Unspecified</pose>\n",
    "  <point>\n",
    "    <x>308</x>\n",
    "    <y>189</y>\n",
    "  </point>\n",
    "</object>\n",
    "<segmented>0</segmented>\n",
    "<size>\n",
    "  <depth>3</depth>\n",
    "  <height>{h}</height>\n",
    "  <width>{w}</width>\n",
    "</size>\n",
    "<source>\n",
    "  <annotation>PASCAL VOC2012</annotation>\n",
    "  <database>The VOC2012 Database</database>\n",
    "  <image>flickr</image>\n",
    "</source>\n",
    "</annotation>\n",
    "  '''.strip()\n",
    "  return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmdet\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def get_bb_contour(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    ## (2) Morph-op to remove noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    ## (3) Find the max-area contour\n",
    "    cnts = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    cnt = sorted(cnts, key=cv2.contourArea)[-1]\n",
    "\n",
    "    ## (4) Crop and save it\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    return x, y, x+w, y+h\n",
    "\n",
    "def get_bb_full_image(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    return 1, 1, img.shape[1], img.shape[0]\n",
    "\n",
    "def get_max_object(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    ## (2) Morph-op to remove noise\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    ## (3) Find the max-area contour\n",
    "    cnts = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "    x1 = []\n",
    "    y1 = []\n",
    "    x2 = []\n",
    "    y2 = []\n",
    "    for cnt in cnts:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        x1.append(x)\n",
    "        y1.append(y)\n",
    "        x2.append(x+w)\n",
    "        y2.append(y+h)\n",
    "    x1 = min(x1)\n",
    "    y1 = min(y1)\n",
    "    x2 = max(x2)\n",
    "    y2 = max(y2)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def create_annotations_and_label_images(cat, query_image_path,\n",
    "                                        label_image_path,\n",
    "                                        ):\n",
    "    color = (0, 0, 255)\n",
    "    thickness = 2\n",
    "    if not os.path.exists(os.path.join(label_image_path, \"img_with_bbox\")):\n",
    "        os.makedirs(os.path.join(label_image_path, \"img_with_bbox\"))\n",
    "\n",
    "    for idx, img_name in enumerate(os.listdir(query_image_path)):\n",
    "        img_path = os.path.join(query_image_path, img_name)\n",
    "        # x1, y1, x2, y2 = get_bb_contour(img_path)\n",
    "        x1, y1, x2, y2 = get_max_object(img_path)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\n",
    "        new_name = img_name.split(\".\")[0] + \"_bbox.\" + img_name.split(\".\")[1]\n",
    "        new_img_path = os.path.join(label_image_path, \"img_with_bbox\", new_name)\n",
    "        cv2.imwrite(new_img_path, img)\n",
    "        xml_content = get_xml_content(cat, img_path, x1, y1, x2, y2)\n",
    "        xml_name = img_name.split(\".\")[0]\n",
    "        if os.path.exists(os.path.join(xml_path, xml_name + \".xml\")):\n",
    "            os.remove(os.path.join(xml_path, xml_name + \".xml\"))\n",
    "        f = open(os.path.join(xml_path, xml_name + \".txt\"), 'w')\n",
    "        f.write(xml_content)\n",
    "        f.close()\n",
    "        os.rename(os.path.join(xml_path, xml_name + \".txt\"), os.path.join(xml_path, xml_name + \".xml\"))\n",
    "        \n",
    "        shutil.copy(os.path.join(query_image_path, img_name), os.path.join(image_xml_path, img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_category(cat):\n",
    "    with open('/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/mmfewshot/detection/datasets/voc.py', 'r') as file:\n",
    "        file_content = file.readlines()\n",
    "\n",
    "    file_content[23] = f\"\\t\\t\\t\\t\\t'pottedplant', 'sheep', 'train', 'tvmonitor', '{cat}'),\\n\"\n",
    "    file_content[33] = f\"\\tNOVEL_CLASSES_SPLIT1=('{cat}',),\\n\"\n",
    "\n",
    "    with open('/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/mmfewshot/detection/datasets/voc.py', 'w') as file:\n",
    "        file.writelines(file_content)\n",
    "    print(file_content[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_image_to_txt_path(cat, query_image_path):\n",
    "  for shot in shots:\n",
    "      if (len(os.listdir(query_image_path))) < shot:\n",
    "          print(f\"[INFO] not enough image for {shot} shot\")\n",
    "          break\n",
    "      print(shot)\n",
    "      f = open(os.path.join(few_shot_ann_path, f\"benchmark_{str(shot)}shot/box_{str(shot)}shot_{cat}_train.txt\"), \"w\")\n",
    "      for img_name in os.listdir(query_image_path)[:shot]:\n",
    "        f.write(f\"VOC2012/JPEGImages/{img_name}\\n\")\n",
    "      f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "data_dict = [\n",
    "    # {\n",
    "    #     'cat': '3326',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3326/focus_3326\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3326/3326\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3326/\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3191',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3191/focus_3191\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3191/3191\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3191/\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': 'door_focus_5_no_background',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/img/focus_door/no_background\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/img/door\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/img/\",\n",
    "    # },\n",
    "    {\n",
    "        'cat': 'kettlebell',\n",
    "        'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/image/KETTLEBELL/focus_Kettlebell\",\n",
    "        'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/image/KETTLEBELL/Kettlebell\",\n",
    "        'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/image/KETTLEBELL/\",\n",
    "    },\n",
    "    # {\n",
    "    #     'cat': '3190',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3190/focus_3190\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3190/3190\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3190/\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '2930',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2930/focus_2930\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2930/2930\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2930/\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '2864',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2864/focus_2864\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2864/2864\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2864/\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3264',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3264/focus_3264\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3264/3264\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3264/\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': 'papasan',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/image/GAR-514/focus_GAR514\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/image/GAR-514/GAR514\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/image/GAR-514\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': 'tv_stand',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2052/focus_2052\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2052/2052\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2052\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': 'tv_stand_2053',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2053/focus_2053\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2053/2053\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2053\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3884',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3884/focus_3884\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3884/3884\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3884\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3883',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3883/focus_3883\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3883/3883\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3883\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3848',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3848/focus_3848\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3848/3848\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3848\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3963',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3963/focus_3963/focus_1\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3963/3963\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3963\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3865',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3865/focus_1\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3865/3865\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3865\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3846',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3846/focus_1\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3846/3846\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3846\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '3771',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3771/focus_3771\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3771/3771\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/3771\",\n",
    "    # },\n",
    "    # {\n",
    "    #     'cat': '2030',\n",
    "    #     'query_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2030/focus_2030\",\n",
    "    #     'database_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2030/2030\",\n",
    "    #     'label_image_path': \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/2030\",\n",
    "    # },\n",
    "]\n",
    "\n",
    "work_dir = \"work_dirs/additional_focus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data  kettlebell ...\n",
      "\tNOVEL_CLASSES_SPLIT1=('kettlebell',),\n",
      "\n",
      "5\n",
      "CUDA_VISIBLE_DEVICES=0 python tools/detection/train.py configs/detection/attention_rpn/voc/split1/attention-rpn_r50_c4_voc-split1_5shot-fine-tuning.py --work-dir work_dirs/additional_focus/kettlebell --no-validate\n"
     ]
    }
   ],
   "source": [
    "for data in data_dict:\n",
    "  print(\"Processing data \", data['cat'], \"...\")\n",
    "  create_annotations_and_label_images(data['cat'], data['query_image_path'],\n",
    "                                      data['label_image_path'])\n",
    "  set_category(data['cat'])\n",
    "  add_image_to_txt_path(data['cat'], data['query_image_path'])\n",
    "  command = f\"CUDA_VISIBLE_DEVICES=0 python tools/detection/train.py configs/detection/attention_rpn/voc/split1/attention-rpn_r50_c4_voc-split1_5shot-fine-tuning.py --work-dir {os.path.join(work_dir, data['cat'])} --no-validate\"\n",
    "  print(command)\n",
    "  # result = subprocess.run(command, shell=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time: 7m 42.7s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"tools/detection/train.py\", line 237, in <module>\n",
      "    main()\n",
      "  File \"tools/detection/train.py\", line 105, in main\n",
      "    cfg = Config.fromfile(args.config)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/mmfewshot/lib/python3.8/site-packages/mmcv/utils/config.py\", line 340, in fromfile\n",
      "    cfg_dict, cfg_text = Config._file2dict(filename,\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/mmfewshot/lib/python3.8/site-packages/mmcv/utils/config.py\", line 183, in _file2dict\n",
      "    check_file_exist(filename)\n",
      "  File \"/home/tanluuuuuuu/miniconda3/envs/mmfewshot/lib/python3.8/site-packages/mmcv/utils/path.py\", line 23, in check_file_exist\n",
      "    raise FileNotFoundError(msg_tmpl.format(filename))\n",
      "FileNotFoundError: file \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/tv_stand_attention-rpn_5shot-fine-tuning/tv_stand_attention-rpn_5shot-fine-tuning.py\" does not exist\n"
     ]
    }
   ],
   "source": [
    "command = f\"CUDA_VISIBLE_DEVICES=0 python tools/detection/train.py /home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/tv_stand_attention-rpn_5shot-fine-tuning/tv_stand_attention-rpn_5shot-fine-tuning.py --no-validate\"\n",
    "result = subprocess.run(command, shell=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['door', '2930', 'kettlebell', '2864', '3190', '3191', '3264', '3326']\n"
     ]
    }
   ],
   "source": [
    "folder_workdir = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/pipeline_v3/test_default_config_max_object\"\n",
    "print(os.listdir(folder_workdir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmfewshot.detection.apis import init_detector, inference_detector\n",
    "import mmcv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "THR = 0.5\n",
    "THR_NMS = 0.2\n",
    "\n",
    "folder_workdir = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/meta-rcnn\"\n",
    "save_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_v4/meta-rcnn\"\n",
    "for data in data_dict:\n",
    "  # Specify the path to model config and checkpoint file\n",
    "  config_file = os.path.join(folder_workdir, data['cat'], \"meta-rcnn_r50_c4_8xb4_voc-split1_5shot-fine-tuning.py\")\n",
    "  checkpoint_file = os.path.join(folder_workdir, data['cat'], \"iter_800.pth\")\n",
    "\n",
    "  # build the model from a config file and a checkpoint file\n",
    "  model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "\n",
    "  img_path = data['database_image_path']\n",
    "  cat = data['cat']\n",
    "  # breakpoint()\n",
    "  num_iter = int(checkpoint_file.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1])\n",
    "  folder_name = f\"{cat}_{num_iter}iter_thr{str(int(THR*100))}_nms{str(int(THR_NMS*100))}\"\n",
    "  print(folder_name)\n",
    "  include_path = os.path.join(save_path, folder_name, \"include\")\n",
    "  if not os.path.exists(include_path):\n",
    "    print(f\"Make dir {include_path}\")\n",
    "    os.makedirs(include_path)\n",
    "\n",
    "  exclude_path = os.path.join(save_path, folder_name, \"exclude\")\n",
    "  if not os.path.exists(exclude_path):\n",
    "    print(f\"Make dir {exclude_path}\")\n",
    "    os.makedirs(exclude_path)\n",
    "\n",
    "  sum_time = 0\n",
    "\n",
    "  def nms(boxes):\n",
    "    boxbox = torch.from_numpy(boxes[:, 0:4])\n",
    "    score = torch.from_numpy(boxes[:, 4])\n",
    "    result = torchvision.ops.nms(boxbox, score, THR_NMS)\n",
    "    return result.numpy()\n",
    "\n",
    "  for im_name in tqdm(os.listdir(img_path)[:500]):\n",
    "    img = os.path.join(img_path, im_name)\n",
    "    st = time.time()\n",
    "    try:\n",
    "      result = inference_detector(model, img)\n",
    "    except:\n",
    "      print(im_name)\n",
    "      continue\n",
    "    sum_time += (time.time() - st)\n",
    "    img = cv2.imread(img)\n",
    "    boxes = np.array([box for box in result[-1] if box[4] >= THR])\n",
    "    if len(boxes) > 0:\n",
    "      box_idxs = nms(boxes)\n",
    "      for box in boxes[box_idxs]:\n",
    "        score = box[4]\n",
    "        if score >= THR:\n",
    "          box = box.astype(int)\n",
    "          color = (0, 0, 255)\n",
    "          thickness = 2\n",
    "          img = cv2.rectangle(img, (box[0], box[1]), (box[2], box[3]), color, thickness)\n",
    "          img = cv2.putText(img, str(score), (box[0], box[1] + 20), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA)\n",
    "      cv2.imwrite(os.path.join(include_path, im_name), img)\n",
    "    else:\n",
    "      cv2.imwrite(os.path.join(exclude_path, im_name), img)\n",
    "  print(f\"Infer on {len(os.listdir(img_path))} got {sum_time}s\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU RAM inference: 2074MiB\n",
    "\n",
    "Time: 17m 22.1s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MISCELLANEOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# Created by Silencer @ Stackoverflow \n",
    "# 2018.01.23 14:41:42 CST\n",
    "# 2018.01.23 18:17:42 CST\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## (1) Convert to gray, and threshold\n",
    "img = cv2.imread(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/photo_2023-04-11_08-17-05.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "th, threshed = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "## (2) Morph-op to remove noise\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11,11))\n",
    "morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "## (3) Find the max-area contour\n",
    "cnts = cv2.findContours(morphed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "cnt = sorted(cnts, key=cv2.contourArea)[-1]\n",
    "\n",
    "## (4) Crop and save it\n",
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "# dst = img[y:y+h, x:x+w]\n",
    "color = (255, 0, 0)\n",
    "thickness = 1\n",
    "img = cv2.rectangle(img, (y, x), (y+h, x+w), color, thickness)\n",
    "cv2.imshow(\"img.jpg\", img)\n",
    "# cv2.imwrite(\"001.png\", dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmdet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "folder1 = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_v2/fine_tune_v0/door_new_bbox_4500iter_thr20/include\"\n",
    "folder2 = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_v2/fine_tune_v0/door_new_bbox_4500iter_th10_nms40/include\"\n",
    "\n",
    "list_1 = os.listdir(folder1)\n",
    "list_2 = os.listdir(folder2)\n",
    "\n",
    "diff_folder = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/pipeline_v2/diff_20_10\"\n",
    "\n",
    "list_diff = list(set(list_2) - set(list_1))\n",
    "for im in list_diff:\n",
    "    source = os.path.join(folder2, im)\n",
    "    dest = os.path.join(diff_folder, im)\n",
    "    shutil.copy(source, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = map(int, input().split())\n",
    "def recur(a, b):\n",
    "    if b == 0:\n",
    "        return a\n",
    "    return recur(b, b % a)\n",
    "c = recur(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/pipeline_v3/test_default_config_max_object/door/iter_4500.pth\n",
      "[array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([], shape=(0, 5), dtype=float32), array([[  0.       ,  16.0888   , 237.56364  , 249.24487  ,   0.9340162]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from mmfewshot.detection.apis import init_detector, inference_detector\n",
    "import cv2\n",
    "\n",
    "config_file = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/pipeline_v3/test_default_config_max_object/door/fsce_r101_fpn_contrastive-loss_voc-split1_5shot-fine-tuning.py\"\n",
    "checkpoint_file = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/work_dirs/pipeline_v3/test_default_config_max_object/door/iter_4500.pth\"\n",
    "\n",
    "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
    "# print(model)\n",
    "img_path = \"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/door_attention_rpn/exclude/B096LGD5BR.jpg\"\n",
    "result = inference_detector(model, img_path)\n",
    "print(result)\n",
    "# img = cv2.imread(img_path)\n",
    "# print(img.shape)\n",
    "# for box in result[-1]:\n",
    "#   if box[-1] >= 0.5:\n",
    "#     box = list(map(int, box))\n",
    "#     print(box)\n",
    "#     cropped = img[box[1]:box[3], box[0]:box[2]]\n",
    "#     cv2.imwrite(\"001.png\", cropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250, 3)\n",
      "(250, 250, 3)\n",
      "[155 182 203]\n",
      "[143 166 181]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "img1 = cv2.imread(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/mmfewshot/result/attention_rpn/door_40thr_attention_rpn/exclude/B0BKTFJ5SY.jpg\")\n",
    "img2 = cv2.imread(\"/home/tanluuuuuuu/Desktop/luunvt/image_retrieval/data/img/door/B0BKTFJ5SY.jpg\")\n",
    "print(img1.shape)\n",
    "print(img2.shape)\n",
    "print(img1[0][0])\n",
    "print(img2[0][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmfewshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2089cf5fb82f7c9ed8e960b0a74486a18e89933162b9e2b46e49851fa4e1e0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
